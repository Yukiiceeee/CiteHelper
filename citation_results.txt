论文标题：Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications
该论文被引用次数：77
该论文被检索到的引用文献：
1. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment
2. The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment
3. [PDF] Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment
4. A survey on lora of large language models
5. A Framework to Implement 1+ N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm
6. Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms
7. Large language model distilling medication recommendation model
8. Higher layers need more lora experts
9. Loraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild
10. Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence
11. Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent
12. CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model
13. Parameter-efficient fine-tuning for large models: A comprehensive survey
14. Heterogeneous contrastive learning for foundation models and beyond
15. Facial affective behavior analysis with instruction tuning
16. MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts
17. Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts
18. HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning
19. [PDF] Empowering Emotional Support Chatbots with Large Language Models
20. PhysMLE: Generalizable and Priors-Inclusive Multi-task Remote Physiological Measurement
21. Efficient training and inference: Techniques for large language models using llama
22. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
23. Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item
24. CorDA: Context-Oriented Decomposition Adaptation of Large Language Models
25. A Parameter-efficient Language Extension Framework for Multilingual ASR
26. RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding
27. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models
28. Retrieval-augmented mixture of lora experts for uploadable machine learning
29. A survey on mixture of experts
30. Improving Abstractive Summarization with Unsupervised Dynamic LoRA Mixtures
31. Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models
32. IterClean: An Iterative Data Cleaning Framework with Large Language Models
33. RoDE: Linear Rectified Mixture of Diverse Experts for Food Large Multi-Modal Models
34. AI Act for the Working Programmer
35. Identity-driven hierarchical role-playing agents
36. MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a Mixture of Dyadic Experts
37. Enhancing Security in Modern Medical Devices: The MEDICALHARM Methodology and CyberLlama2
38. LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin
39. Multi-LoRA Fine-Tuned Segment Anything Model for Urban Man-Made Object Extraction
40. Efficient Mixture of Experts based on Large Language Models for Low-Resource Data Preprocessing
41. Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models
42. Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges
43. Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering
44. Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE
45. HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models
46. MERGING LORAS LIKE PLAYING LEGO: PUSH
47. DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism
48. MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning
49. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
50. DEeR: Deviation Eliminating and Noise Regulating for Privacy-preserving Federated Low-rank Adaptation
51. MoR: Mixture of Ranks for Low-Rank Adaptation Tuning
52. MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning
53. MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning
54. Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies
55. A Survey of Small Language Models
56. Reparameterization-Based Parameter-Efficient Fine-Tuning Methods for Large Language Models: A Systematic Survey
57. CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning
58. Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization
59. From General to Specific: Utilizing General Hallucation to Automatically Measure the Role Relationship Fidelity for Specific Role-Play Agents
60. Separable Mixture of Low-Rank Adaptation for Continual Visual Instruction Tuning
61. Separable Mixture of Low-Rank Adaptation for Continual Visual Instruction Tuning
62. BoRA: Bi-dimensional Weight-Decomposed Low-Rank Adaptation
63. Advancing Single-and Multi-task Text Classification through Large Language Model Fine-tuning
64. MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning
65. A Survey on Inference Optimization Techniques for Mixture of Experts Models
66. GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE
67. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
68. The Scaling Law for LoRA Base on Mutual Information Upper Bound
69. [PDF] Exploring the Landscape of Large and Small Language Models: Advancements, Trade-offs, and Future Directions
70. OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal Finetuning
71. Adapters Selector: Cross-domains and Multi-tasks LoRA Modules Integration Usage Method
72. Parameter-Efficient Fine-Tuning for Foundation Models
73. Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning
74. Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM

论文标题：Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models
该论文被引用次数：29
该论文被检索到的引用文献：
1. Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent
2. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
3. Using Contrastive Learning with Generative Similarity to Learn Spaces that Capture Human Inductive Biases
4. Unlocking continual learning abilities in language models
5. A survey on mixture of experts
6. Teamlora: Boosting low-rank adaptation with expert collaboration and competition
7. Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging
8. DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism
9. Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs
10. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
11. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
12. A Survey of Small Language Models
13. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
14. Iterative Large Language Models Evolution through Self-Critique
15. PERFT: Parameter-Efficient Routed Fine-Tuning for Mixture-of-Expert Model
16. Iterative Large Language Models Evolution through Self-Critique
17. Аналіз настроїв текстових повідомлень із використанням методів машинного навчання
18. 法律案件要素识别混合专家大模型.
19. Advancing Single-and Multi-task Text Classification through Large Language Model Fine-tuning
20. MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning
21. A Survey on Inference Optimization Techniques for Mixture of Experts Models
22. GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE
23. VELoRA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition
24. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
25. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
26. GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism
27. MoKA: Parameter Efficiency Fine-Tuning via Mixture of Kronecker Product Adaption
28. MoKA: Parameter Efficiency Fine-Tuning via Mixture of Kronecker Product Adaption
29. A Survey on Mixture of Experts: Advancements, Challenges, and Future Directions

论文标题：LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin
该论文被引用次数：9
该论文被检索到的引用文献：
1. A survey on lora of large language models
2. Visual Attention Estimation Algorithm and Dynamic Neural Network Based Object Detection for Intelligent Vehicles
3. CorDA: Context-Oriented Decomposition Adaptation of Large Language Models
4. The Evolution of Mixture of Experts: A Survey from Basics to Breakthroughs
5. MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing
6. CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance
7. HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models
8. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models
9. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality

论文标题：Higher layers need more lora experts
该论文被引用次数：36
该论文被检索到的引用文献：
1. [PDF] MoSA: Mixture of Sparse Adapters for Visual Efficient Tuning
2. A survey on lora of large language models
3. Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts
4. 基于联邦分割学习与低秩适应的RoBERTa 预训练模型微调方法.
5. AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts
6. MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors
7. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
8. MLAE: Masked LoRA Experts for Parameter-Efficient Fine-Tuning
9. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models
10. Retrieval-augmented mixture of lora experts for uploadable machine learning
11. A survey on mixture of experts
12. Low-Rank Interconnected Adaptation Across Layers
13. [PDF] MoBA: Mixture of Bi-directional Adapter for Multi-modal Sarcasm Detection
14. Teamlora: Boosting low-rank adaptation with expert collaboration and competition
15. DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism
16. SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture
17. AT-MoE: Adaptive Task-planning Mixture of Experts via LoRA Approach
18. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
19. Towards Optimal Adapter Placement for Efficient Transfer Learning
20. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
21. What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective
22. Reparameterization-Based Parameter-Efficient Fine-Tuning Methods for Large Language Models: A Systematic Survey
23. Sparse Mixture of Experts Language Models Excel in Knowledge Distillation
24. Sparse Mixture of Experts Language Models Excel in Knowledge Distillation
25. Reparameterization-Based Parameter-Efficient Fine-Tuning Methods for Large Language Models: A Systematic Survey
26. Advancing Single-and Multi-task Text Classification through Large Language Model Fine-tuning
27. MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning
28. A Survey on Inference Optimization Techniques for Mixture of Experts Models
29. GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE
30. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
31. An intelligent prediction paradigm for milling tool parameters design based on multi-task tabular data deep transfer learning integrating physical knowledge
32. GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism
33. Adapters Selector: Cross-domains and Multi-tasks LoRA Modules Integration Usage Method
34. GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism
35. Adapters Selector: Cross-domains and Multi-tasks LoRA Modules Integration Usage Method
36. Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning

论文标题：MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning
该论文被引用次数：23
该论文被检索到的引用文献：
1. Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine
2. Accurate and Efficient Fine-Tuning of Quantized Large Language Models Through Optimal Balance
3. Open Llama2 Model for the Lithuanian Language
4. DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model
5. PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning
6. Pear: Pruning and Sharing Adapters in Visual Parameter-Efficient Fine-Tuning
7. FROM LOW TO ARBITRARY RANK
8. SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture
9. Prompt Compression for Large Language Models: A Survey
10. FROM LOW TO ARBITRARY RANK
11. SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture
12. Prompt Compression for Large Language Models: A Survey
13. [PDF] Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables
14. SFT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity
15. Learning from" Silly" Questions Improves Large Language Models, But Only Slightly
16. BoRA: Bi-dimensional Weight-Decomposed Low-Rank Adaptation
17. Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables.
18. CURing Large Models: Compression via CUR Decomposition
19. Tensor Product Attention Is All You Need
20. Fine Tuning without Catastrophic Forgetting via Selective Low Rank Adaptation
21. Fine Tuning without Catastrophic Forgetting via Selective Low Rank Adaptation
22. I3S: Importance Sampling Subspace Selection for Low-Rank Optimization in LLM Pretraining

论文标题：Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts
该论文被引用次数：37
该论文被检索到的引用文献：
1. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
2. Aligning to thousands of preferences via system message generalization
3. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models
4. A survey on mixture of experts
5. Affective computing in the era of large language models: A survey from the nlp perspective
6. MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a Mixture of Dyadic Experts
7. Teamlora: Boosting low-rank adaptation with expert collaboration and competition
8. Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models
9. Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges
10. HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models
11. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models
12. DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism
13. Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs
14. SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture
15. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
16. MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases
17. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
18. MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases
19. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
20. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
21. AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment
22. Rethinking Strategic Mechanism Design In The Age Of Large Language Models: New Directions For Communication Systems
23. DataLab: A Unifed Platform for LLM-Powered Business Intelligence
24. Customize Segment Anything Model for Multi-Modal Semantic Segmentation with Mixture of LoRA Experts
25. A Unified Convergence Theory for Large Language Model Efficient Fine-tuning
26. MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning
27. " They've Stolen My GPL-Licensed Model!": Toward Standardized and Transparent Model Licensing
28. GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE
29. Trustworthy and Efficient LLMs Meet Databases
30. Optimizing Large Language Models with an Enhanced LoRA Fine-Tuning Algorithm for Efficiency and Robustness in NLP Tasks
31. GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism
32. OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal Finetuning
33. Adapters Selector: Cross-domains and Multi-tasks LoRA Modules Integration Usage Method
34. Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning
35. Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning
36. On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation
37. Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning

论文标题：Mixture-of-Subspaces in Low-Rank Adaptation
该论文被引用次数：10
该论文被检索到的引用文献：
1. Low-Rank Interconnected Adaptation Across Layers
2. MoR: Mixture of Ranks for Low-Rank Adaptation Tuning
3. MoR: Mixture of Ranks for Low-Rank Adaptation Tuning
4. Linear Chain Transformation: Expanding Optimization Dynamics for Fine-Tuning Large Language Models
5. Linear Chain Transformation: Expanding Optimization Dynamics for Fine-Tuning Large Language Models
6. Linear Chain Transformation: Expanding Optimization Dynamics for Fine-Tuning Large Language Models
7. Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual Learning Vision-Language Models with Dynamic Rank-Selective LoRA
8. Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning
9. GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism
10. [PDF] Low-Rank Adaptation for Scalable Fine-Tuning of Pre-Trained Language Models

论文标题：MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning
该论文被引用次数：37
该论文被检索到的引用文献：
1. A survey on lora of large language models
2. Prompt-saw: Leveraging relation-aware graphs for textual prompt compression
3. Dialectical alignment: Resolving the tension of 3h and security threats of llms
4. Multi-hop question answering under temporal knowledge editing
5. Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts
6. MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models
7. Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top
8. Editable Concept Bottleneck Models
9. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
10. Auto-selected Knowledge Adapters for Lifelong Person Re-identification
11. Towards Lifelong Learning of Large Language Models: A Survey
12. CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance
13. HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models
14. [PDF] Recent Advances in Robot Navigation via Large Language Models: A Review
15. DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism
16. Understanding Reasoning in Chain-of-Thought from the Hopfieldian View
17. Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs
18. AT-MoE: Adaptive Task-planning Mixture of Experts via LoRA Approach
19. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
20. MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning
21. A Survey of Small Language Models
22. Towards Multi-dimensional Explanation Alignment for Medical Classification
23. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
24. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
25. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
26. Advancing Single-and Multi-task Text Classification through Large Language Model Fine-tuning
27. GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE
28. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
29. The Scaling Law for LoRA Base on Mutual Information Upper Bound
30. [PDF] Exploring the Landscape of Large and Small Language Models: Advancements, Trade-offs, and Future Directions
31. [PDF] Exploring the Landscape of Large and Small Language Models: Advancements, Trade-offs, and Future Directions
32. [PDF] Exploring the Landscape of Large and Small Language Models: Advancements, Trade-offs, and Future Directions
33. Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning
34. [PDF] Low-Rank Adaptation for Scalable Fine-Tuning of Pre-Trained Language Models
35. Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning

论文标题：Mixture of cluster-conditional lora experts for vision-language instruction tuning
该论文被引用次数：65
该论文被检索到的引用文献：
1. [PDF] MoSA: Mixture of Sparse Adapters for Visual Efficient Tuning
2. [PDF] TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models
3. Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks
4. A survey on lora of large language models
5. Moe-llava: Mixture of experts for large vision-language models
6. Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms
7. Learning to route among specialized experts for zero-shot generalization
8. CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning
9. Mope-clip: Structured pruning for efficient vision-language models with module-wise pruning error metric
10. Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation
11. LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
12. Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning
13. Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts
14. Automated evaluation of large vision-language models on self-driving corner cases
15. Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment
16. AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts
17. Towards modular llms by building and reusing a library of loras
18. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
19. Exploring Training on Heterogeneous Data with Mixture of Low-rank Adapters
20. Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts
21. A survey on mixture of experts
22. Lemoe: Advanced mixture of experts adaptor for lifelong model editing of large language models
23. Mome: Mixture of multimodal experts for generalist multimodal large language models
24. RoDE: Linear Rectified Mixture of Diverse Experts for Food Large Multi-Modal Models
25. Not All Attention is Needed: Parameter and Computation Efficient Tuning for Multi-modal Large Language Models via Effective Attention Skipping
26. Q-MoE: Connector for MLLMs with Text-Driven Routing
27. EEGMamba: Bidirectional State Space Models with Mixture of Experts for EEG Classification
28. Harmonizing Visual Text Comprehension and Generation
29. Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators
30. CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance
31. Emova: Empowering language models to see, hear and speak with vivid emotions
32. Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE
33. DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism
34. Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs
35. AT-MoE: Adaptive Task-planning Mixture of Experts via LoRA Approach
36. Improving General Text Embedding Model: Tackling Task Conflict and Data Imbalance through Model Merging
37. ViMoE: An Empirical Study of Designing Vision Mixture-of-Experts
38. MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning
39. ViMoE: An Empirical Study of Designing Vision Mixture-of-Experts
40. MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning
41. A Survey of Small Language Models
42. Q-MoE: Connector for MLLMs with Text-Driven Routing
43. Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models
44. A Survey of Small Language Models
45. Q-MoE: Connector for MLLMs with Text-Driven Routing
46. Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models
47. Separable Mixture of Low-Rank Adaptation for Continual Visual Instruction Tuning
48. 法律案件要素识别混合专家大模型.
49. Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models by Recycling Pre-Tuned LoRAs
50. SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts
51. Advancing Single-and Multi-task Text Classification through Large Language Model Fine-tuning
52. GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE
53. RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting
54. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
55. Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models
56. [PDF] Exploring the Landscape of Large and Small Language Models: Advancements, Trade-offs, and Future Directions
57. Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models
58. [PDF] Exploring the Landscape of Large and Small Language Models: Advancements, Trade-offs, and Future Directions
59. GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism
60. Improved Sparse Upcycling for Instruction Tuning
61. HotMoE: Exploring Sparse Mixture-of-Experts for Hyperspectral Object Tracking
62. MoSCE-ReID: Mixture of semantic clustering experts for person re-identification

论文标题：Mixture-of-loras: An efficient multitask tuning for large language models
该论文被引用次数：36
该论文被检索到的引用文献：
1. Parameter-efficient fine-tuning for large models: A comprehensive survey
2. Trends and challenges of real-time learning in large language models: A critical review
3. Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment
4. [PDF] Towards Incremental Learning in Large Language Models: A Critical Review
5. MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models
6. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
7. Customising generative AI: Harnessing document retrieval and fine-tuning alternatives for dynamic marketing insights
8. Wings: Learning Multimodal LLMs without Text-only Forgetting
9. Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks
10. Retrieval-augmented mixture of lora experts for uploadable machine learning
11. MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a Mixture of Dyadic Experts
12. Large Language Models for Programming Industrial Control Systems and Mitigating Real-World Software Vulnerabilities
13. Teamlora: Boosting low-rank adaptation with expert collaboration and competition
14. Adaptive Quantization Error Reconstruction for LLMs with Mixed Precision
15. Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models
16. Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges
17. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models
18. DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism
19. Scalable Multi-Domain Adaptation of Language Models using Modular Experts
20. Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants
21. LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks
22. MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases
23. MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases
24. A Survey of Small Language Models
25. V-LoRA: An Efficient and Flexible System Boosts Vision Applications with LoRA LMM
26. V-LoRA: An Efficient and Flexible System Boosts Vision Applications with LoRA LMM
27. Advancing Single-and Multi-task Text Classification through Large Language Model Fine-tuning
28. MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning
29. FineGates: LLMs Finetuning with Compression using Stochastic Gates
30. VELoRA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition
31. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
32. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
33. LLM4CVE: Enabling Iterative Automated Vulnerability Repair with Large Language Models
34. Adapters Selector: Cross-domains and Multi-tasks LoRA Modules Integration Usage Method
35. Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning
36. Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning

论文标题：LoRAMoE: Alleviating world knowledge forgetting in large language models via MoE-style plugin
该论文被引用次数：20
该论文被检索到的引用文献：
1. A survey on lora of large language models
2. Visual Attention Estimation Algorithm and Dynamic Neural Network Based Object Detection for Intelligent Vehicles
3. CorDA: Context-Oriented Decomposition Adaptation of Large Language Models
4. The Evolution of Mixture of Experts: A Survey from Basics to Breakthroughs
5. MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing
6. CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance
7. HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models
8. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models
9. AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality
10. MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning
11. CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning
12. CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning
13. EC-PEFT: An Expertise-Centric Parameter-Efficient Fine-Tuning Framework for Large Language Models
14. Аналіз настроїв текстових повідомлень із використанням методів машинного навчання
15. Advancing Single-and Multi-task Text Classification through Large Language Model Fine-tuning
16. A Survey on Inference Optimization Techniques for Mixture of Experts Models
17. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
18. The Scaling Law for LoRA Base on Mutual Information Upper Bound
19. Adapters Selector: Cross-domains and Multi-tasks LoRA Modules Integration Usage Method
20. MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs

论文标题：CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control and Altering of T2I Models
该论文被引用次数：4
该论文被检索到的引用文献：
1. Training-Free Style and Content Transfer by Leveraging U-Net Skip Connections in Stable Diffusion 2.
2. Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations
3. Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations
4. Mechanisms of Projective Composition of Diffusion Models

论文标题：Implicit style-content separation using b-lora
该论文被引用次数：13
该论文被检索到的引用文献：
1. Style-nerf2nerf: 3d style transfer from style-aligned multi-view images
2. A survey on lora of large language models
3. StyleTex: Style Image-Guided Texture Generation for 3D Models
4. FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models
5. UnZipLoRA: Separating Content and Style from a Single Image
6. LoRA. rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation
7. CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training
8. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
9. [PDF] Low-Rank Adaptation for Scalable Fine-Tuning of Pre-Trained Language Models

论文标题：Ziplora: Any subject in any style by effectively merging loras
该论文被引用次数：71
该论文被检索到的引用文献：
1. Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?
2. Controllable generation with text-to-image diffusion models: A survey
3. FSViewFusion: Few-Shots View Generation of Novel Objects
4. LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models
5. Llm merging: Building llms efficiently through merging
6. Synthetic vs real: exploring the impact of synthetic data on medical image classification
7. PhysMLE: Generalizable and Priors-Inclusive Multi-task Remote Physiological Measurement
8. Towards modular llms by building and reusing a library of loras
9. FreeTuner: Any Subject in Any Style with Training-free Diffusion
10. RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control
11. Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts
12. Style-nerf2nerf: 3d style transfer from style-aligned multi-view images
13. Géométrie des vecteurs de tâches pour l'association et la combinaison de modèles
14. Adapt to Scarcity: Few-Shot Deepfake Detection via Low-Rank Adaptation
15. Magmax: Leveraging model merging for seamless continual learning
16. MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a Mixture of Dyadic Experts
17. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities
18. Diffusion-based visual art creation: A survey and new perspectives
19. Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis
20. Multi-modal generative ai: Multi-modal llm, diffusion and beyond
21. MV-Adapter: Multi-view Consistent Image Generation Made Easy
22. Attack on LLMs: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem
23. What Matters for Model Merging at Scale?
24. Low-Rank Continual Personalization of Diffusion Models
25. Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation
26. Deep Linear Probe Generators for Weight Space Learning
27. Efficient diffusion models: A comprehensive survey from principles to practices
28. Lora soups: Merging loras for practical skill composition tasks
29. Representing Model Weights with Language using Tree Experts
30. HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation
31. MultiLoRA: Multi-Directional Low Rank Adaptation for Multi-Domain Recommendation
32. Model merging with SVD to tie the Knots
33. StyleTex: Style Image-Guided Texture Generation for 3D Models
34. Hollowed Net for On-Device Personalization of Text-to-Image Diffusion Models
35. HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation
36. Time-Varying LoRA: Towards Effective Cross-Domain Fine-Tuning of Diffusion Models
37. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning
38. Joint Diffusion models in Continual Learning
39. IterIS: Iterative Inference-Solving Alignment for LoRA Merging
40. Multi LoRA Meets Vision: Merging multiple adapters to create a multi task model
41. LoRA of Change: Learning to Generate LoRA for the Editing Instruction from A Single Before-After Image Pair
42. Safety Alignment Backfires: Preventing the Re-emergence of Suppressed Concepts in Fine-tuned Text-to-Image Diffusion Models
43. Mv-adapter: Multi-view consistent image generation made easy
44. UnZipLoRA: Separating Content and Style from a Single Image
45. LoRA. rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation
46. LoRACLR: Contrastive Adaptation for Customization of Diffusion Models
47. MulSMo: Multimodal Stylized Motion Generation by Bidirectional Control Flow
48. SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation
49. Towards maintainable machine learning development through continual and modular learning
50. Conditional Balance: Improving Multi-Conditioning Trade-Offs in Image Generation
51. Low-Rank Adaptation for Foundation Models: A Comprehensive Review
52. ComMer: a Framework for Compressing and Merging User Data for Personalization
53. DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models
54. Bringing Characters to New Stories: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting
55. Fine Tuning without Catastrophic Forgetting via Selective Low Rank Adaptation
56. HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation

论文标题：UnZipLoRA: Separating Content and Style from a Single Image
该论文被引用次数：0
该论文被检索到的引用文献：

